Reimplement Thud player
✓ C++(11) because of abstraction limitations in C
✓ use game state representation amenable to machine learning (cf Giraffe)
* decouple UI from computer player
* decouple each side's computer players from each other
* enable easy comparison between differing versions of code/parameters
* allow "pondering" - thinking on opponent's turn
* use same code for interactive play, self play learning, subprocess
* proper up-front testing (tdd, quickcheck-type properties, more?)
* proper on-disk format for storing game data (likely sqlite)
✓ build with cmake and ninja

non-goals
* do everything at once

rough task order (TEST ALL THE THINGS)
✓ game state representation
✓ bulk move generation
✓ do move
✓ minimal evaluator
* undo move
* incremental move generation (boost coroutine2? no, too bulky)
* mcts (do this first? less understood, more interesting)
* some kind of game tree debugging tool thingy
* UI of some sort
* naive minimax
* iterative deepening
* zobrist hashing
* ttable
* game data serialization format
* move ordering
* alpha-beta pruning
* search extension (eg for sacrifice moves)
* mtd(f) (for help with ttable debugging)
* fat evaluator
* automatic tuning of fat evaluator

some ideas for simple learning
* remember all game states for games actually played
* also shallow re-search all non-taken moves from actual-play game states to ensure they are in the ttable written to disk
* record how many times game states show up in actual-play games, with the average score at the end of the game, and how recently they appeared in actual-play
* keep count of actual-play games, and age out states without recent plays
* record game states in 8-fold symmetry
